Letâ€™s derive the backpropagation algorithm for a neural network with 
ğ¾
K hidden layers, one input layer, and one output layer. Backpropagation involves two main steps: forward pass to calculate outputs and errors, and backward pass to compute gradients for updating weights. Below is the step-by-step derivation, followed by the pseudocode.

1. Notation
ğ¿
L: Total number of layers (including input and output).
ğ‘§
[
ğ‘™
]
z 
[l]
 : Weighted sum of inputs for layer 
ğ‘™
l.
ğ‘
[
ğ‘™
]
a 
[l]
 : Activation output of layer 
ğ‘™
l.
ğ‘Š
[
ğ‘™
]
W 
[l]
 : Weights connecting layer 
ğ‘™
âˆ’
1
lâˆ’1 to 
ğ‘™
l.
ğ‘
[
ğ‘™
]
b 
[l]
 : Biases for layer 
ğ‘™
l.
ğ›¿
[
ğ‘™
]
Î´ 
[l]
 : Error term (gradient of loss with respect to 
ğ‘§
[
ğ‘™
]
z 
[l]
 ).
ğ‘”
â€²
g 
â€²
 : Derivative of activation function.
ğ½
J: Loss function.
2. Forward Pass
For each layer 
ğ‘™
=
1
,
2
,
â€¦
,
ğ¿
l=1,2,â€¦,L:

Compute the weighted sum:
ğ‘§
[
ğ‘™
]
=
ğ‘Š
[
ğ‘™
]
ğ‘
[
ğ‘™
âˆ’
1
]
+
ğ‘
[
ğ‘™
]
z 
[l]
 =W 
[l]
 a 
[lâˆ’1]
 +b 
[l]
 
Apply activation function:
ğ‘
[
ğ‘™
]
=
ğ‘”
(
ğ‘§
[
ğ‘™
]
)
a 
[l]
 =g(z 
[l]
 )
3. Backward Pass
Step 1: Compute Output Layer Error (
ğ‘™
=
ğ¿
l=L)
For the output layer:

ğ›¿
[
ğ¿
]
=
âˆ‚
ğ½
âˆ‚
ğ‘
[
ğ¿
]
â‹…
ğ‘”
â€²
(
ğ‘§
[
ğ¿
]
)
Î´ 
[L]
 = 
âˆ‚a 
[L]
 
âˆ‚J
â€‹
 â‹…g 
â€²
 (z 
[L]
 )
Step 2: Backpropagate Error to Hidden Layers (
ğ‘™
=
ğ¿
âˆ’
1
,
ğ¿
âˆ’
2
,
â€¦
,
1
l=Lâˆ’1,Lâˆ’2,â€¦,1)
For each hidden layer 
ğ‘™
l:

ğ›¿
[
ğ‘™
]
=
(
ğ‘Š
[
ğ‘™
+
1
]
)
âŠ¤
ğ›¿
[
ğ‘™
+
1
]
â‹…
ğ‘”
â€²
(
ğ‘§
[
ğ‘™
]
)
Î´ 
[l]
 =(W 
[l+1]
 ) 
âŠ¤
 Î´ 
[l+1]
 â‹…g 
â€²
 (z 
[l]
 )
Step 3: Compute Gradients
For each layer 
ğ‘™
=
1
,
2
,
â€¦
,
ğ¿
l=1,2,â€¦,L:

Gradients of weights:
âˆ‚
ğ½
âˆ‚
ğ‘Š
[
ğ‘™
]
=
ğ›¿
[
ğ‘™
]
(
ğ‘
[
ğ‘™
âˆ’
1
]
)
âŠ¤
âˆ‚W 
[l]
 
âˆ‚J
â€‹
 =Î´ 
[l]
 (a 
[lâˆ’1]
 ) 
âŠ¤
 
Gradients of biases:
âˆ‚
ğ½
âˆ‚
ğ‘
[
ğ‘™
]
=
ğ›¿
[
ğ‘™
]
âˆ‚b 
[l]
 
âˆ‚J
â€‹
 =Î´ 
[l]
 
4. Update Weights and Biases
Using gradient descent, update weights and biases:

ğ‘Š
[
ğ‘™
]
=
ğ‘Š
[
ğ‘™
]
âˆ’
ğœ‚
âˆ‚
ğ½
âˆ‚
ğ‘Š
[
ğ‘™
]
W 
[l]
 =W 
[l]
 âˆ’Î· 
âˆ‚W 
[l]
 
âˆ‚J
â€‹
 
ğ‘
[
ğ‘™
]
=
ğ‘
[
ğ‘™
]
âˆ’
ğœ‚
âˆ‚
ğ½
âˆ‚
ğ‘
[
ğ‘™
]
b 
[l]
 =b 
[l]
 âˆ’Î· 
âˆ‚b 
[l]
 
âˆ‚J
â€‹
 
where 
ğœ‚
Î· is the learning rate.
